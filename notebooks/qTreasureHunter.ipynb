{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treasure Hunters Inc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this assignment is to explore the reinforcement learning paradigm for solving (navigating) a map like environment.\n",
    "\n",
    "### Sec-1.1: Description of Map\n",
    "---\n",
    "The map is conceived of as a maze in which walls between cells serve the purpose of **obstacles**. There is **treasure** hidden in one of the cells of the maze. _The goal of **reinforcement learning agent** is to find the best path to the treasure from any cell in the maze while avoiding the obstacles._\n",
    "\n",
    "In the code below, the maze is enclosed in the ``` Board Class```. It is described as a partial adjacency matrix of the cells which are reachable from each other i.e., not separated by a wall. In addition to the cells, the board class contains the location of the cell in which treasure is hidden.\n",
    "\n",
    "### Sec-1.2: Q-Learning Strategy\n",
    "---\n",
    "We use the Q-learning approach to design an AI agent which can solve the board. In our design, the behavior of the Q-agent is governed by two metrics; as is standard in Q-learning pardigm:\n",
    "  - Imediate **reward** associated with an action\n",
    "  - **Quality** of the action; dictated by the history of moves which follow the action - learnt by the agent through experience\n",
    "\n",
    "In the agent class coded below, the class private matrices **R** and **Q** serve as the two metrics described above repectively.\n",
    "\n",
    "The agent learns to navigate the maze iteratively via the ```train ()``` function. During training, the quality matrix ```Q``` is updated to gather **experience** by the agent using the **Bellman Equation**.\n",
    "\n",
    "Once the agent has learnt to navigate the maze, the game is played by giving an arbitrary start point to the agent and checking if the agent can successfully find the treasure from that starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sec-2: Code\n",
    "---\n",
    "Given the complexity of this assignmet, the code is amply commented to help in understanding the flow. The ```Board Class``` contains description of the environment; including the definition of the maze, the obstacles and the location of the treasure. The ```Agent Class``` encodes the behavior of the Q-Learning agent and the meat of the learning algorithm itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Only the following two external libraries are used by our learning alogrithm\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec-2.1: Board Class\n",
    "---\n",
    "The board class describes the environment in which game is played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Board:\n",
    "    def __init__ (self, maze, goal):\n",
    "        # The maze is described as a partial adjacency matrix;\n",
    "        # where each cell index contains the location of the\n",
    "        # next cell which is accessible from it\n",
    "        self.maze = maze\n",
    "        \n",
    "        # Goal annotates the location of the treasure in the\n",
    "        # board. Note that this location is fixed for a given\n",
    "        # setting of the board\n",
    "        self.goal = goal\n",
    "\n",
    "    def get_maze (self):\n",
    "        return self.maze\n",
    "\n",
    "    def get_goal (self):\n",
    "        return self.goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec-2.2: Agent Class\n",
    "---\n",
    "The agent class contain the meat of the Q-learning algorithm. This class is fairly large but we have amply commented the code to aide in understanding.\n",
    "\n",
    "The most significant part of the agent is the **policies** of the game which are encoded in the agent's behavior via the reward (**R**) matrix. For sake of clarity, these policies are described below in detail.\n",
    "\n",
    "#### Sec-2.2.1: Game Policies\n",
    "**R** matrix stands for *rewards*. It specifies the reward for visiting\n",
    "every state from every other state; hence it is an 'N x N' (N = total number of states) matrix.\n",
    "It is the most important matrix for specifying the \"rules\" of the game\n",
    "and consequently in dictating the behavior of the agent when it plays\n",
    "the game. For the assignment at hand, following simple rules are\n",
    "implemented:\n",
    "\n",
    "  - **Policy-1**: If a state can be visited from a given state, the reward is\n",
    "slightly negative i.e., the reward (r) for a \"legitimate\" move in our\n",
    "game is slightly negative (-1 < r < 0).\n",
    "\n",
    "  - **Policy-2**: If a state cannot be visited from a given state (i.e., illegal\n",
    "move), the reward is maximally negative. In the semantics of our game,\n",
    "this is indicated by value '-1'.\n",
    "\n",
    "  - **Policy-3**: The reward for reaching the final state (i.e., goal) is\n",
    "maximally positive; indicated by value '1'.\n",
    "\n",
    "The **rationale for assigning slightly negative reward** to a legitimate move\n",
    "is to discourage the tendency to make unnecessary moves. With this\n",
    "policy, a good agent should be able to find the shortest sequence of\n",
    "moves for reaching the goal.\n",
    "\n",
    "Also, **to discourage the agent from staying in the same state**, the move\n",
    "from a state to itself is considered an illegal move i.e., it is\n",
    "encoded with '-1' reward.\n",
    "\n",
    "#### Sec-2.2.2: Training Strategy\n",
    "  - At the begining of each training cycle, the agent is placed in a\n",
    "   randomly selected cell of the maze.\n",
    "  - The agent randomly selects a next possible state from the given\n",
    "   state.\n",
    "    - From the chosen next state, the \"history\" of max. total reward which\n",
    "         can be obtained from that state is calculated. This is done by\n",
    "         performing a lookup in the Q matrix.\n",
    "    - Using the given state, a possible next state and the maximum reward\n",
    "         obtainable from the next state, the Q matrix is updated using the\n",
    "         Bellman equation:\n",
    "         ```Q [state][nextState] = R [state][nextState] + gamma * max_quality_next_state```\n",
    "    - Current state is updated to next state and the above strategy\n",
    "         is continued until the end state is reached.\n",
    "  - At the end of each training cycle, the improvement in the Q matrix\n",
    "   is measured. Training is terminated when a cycle does not yeild\n",
    "   signficant improvement.\n",
    "\n",
    "Once training is complete, the agent can be used to navigate the maze\n",
    "on its own given arbitrary starting points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A global variable to dictate the exploration count for the Q-agent\n",
    "EXPLORE_COUNT = 10\n",
    "\n",
    "class QLearn:\n",
    "    def __init__ (self, board):\n",
    "        self.R = None\n",
    "        self.Q = None\n",
    "        self.maze = board.get_maze ()\n",
    "        self.rows = len (self.maze)\n",
    "        self.cols = len (self.maze [0])\n",
    "        self.states = self.rows * self.cols\n",
    "        self.treasure = board.get_goal ()\n",
    "        self.endState = self.cell_to_state (self.treasure)\n",
    "\n",
    "        # The following hash table will contain every state the can be\n",
    "        # \"legally\" visited from a given state. It will be keyed with the\n",
    "        # state-value and will contain list of next state-values for the\n",
    "        # given state\n",
    "        self.nextStates = {}\n",
    "\n",
    "        self.initialize ()\n",
    "        return\n",
    "\n",
    "    def initialize (self):\n",
    "        self.__initQ ()\n",
    "        self.__initR ()\n",
    "        return\n",
    "\n",
    "    def __initQ (self):\n",
    "        ''' Q matrix contains history of visiting every state from every other\n",
    "        state. In reinforcement learning, it serves the purpose of tracking the\n",
    "        \"Quality\" of making a move given the history of play. Provided that we\n",
    "        have 'N' states, Q matrix will contain 'N x N' elements. Here we\n",
    "        intiialize all these elements to zero since our agent has not played\n",
    "        any games yet and hence it has no hisotry of moves.'''\n",
    "        \n",
    "        self.Q = np.zeros ((self.states, self.states))\n",
    "        return\n",
    "\n",
    "    def __initR (self):\n",
    "        ''' R matrix stands for rewards. It specifies the reward for visiting\n",
    "        every state from every other state; hence it is also an 'N x N' matrix.\n",
    "        It is the most important matrix for specifying the \"rules\" of the game\n",
    "        and consequently in dictating the behavior of the agent when it plays\n",
    "        the game. These rules are described in the documentation in detail.'''\n",
    "\n",
    "        # Initialize R matrix for all states with '-1'; indicating that\n",
    "        # no other state is accessbile from any state\n",
    "        self.R = np.full ((self.states, self.states), -1, dtype = np.float)\n",
    "\n",
    "        for x in range (self.rows):\n",
    "            for y in range (self.cols):\n",
    "                cell = (x, y)\n",
    "                state = self.cell_to_state (cell)\n",
    "                adjCell = self.maze [x][y]\n",
    "                adjState = self.cell_to_state (adjCell)\n",
    "\n",
    "                if state == adjState:\n",
    "                    continue\n",
    "\n",
    "                self.link_states (state, adjState)\n",
    "\n",
    "                # If the cell next to us is the goal cell, the reward is max.\n",
    "                if adjState == self.endState:\n",
    "                    self.R [state][adjState] = 1.0\n",
    "                    continue\n",
    "\n",
    "                # Same check as above but from the goal cell's PoV\n",
    "                if state == self.endState:\n",
    "                    self.R [adjState][state] = 1.0\n",
    "                    continue\n",
    "\n",
    "                # States are reachable from each other\n",
    "                self.R [state][adjState] = -0.1\n",
    "                self.R [adjState][state] = -0.1\n",
    "\n",
    "        # We don't want to come out of end state so we reward the action of\n",
    "        # 'staying in the end state' with the maximum possible value\n",
    "        self.R [self.endState][self.endState] = 1.0\n",
    "\n",
    "        return\n",
    "\n",
    "    def train (self, gamma = 0.8, improvement_threshold = 0.001):\n",
    "        ''' The purpose of train function is to use reinforcement learning to\n",
    "        train the AI for navigating the maze in the best possible way as per\n",
    "        the rules of the game. The training strategy is described in detail\n",
    "        in the documentation'''\n",
    "        print \"[STATUS] Training Q-agent...\"\n",
    "        epoch = 0\n",
    "\n",
    "        while True:\n",
    "            epoch += 1\n",
    "            prevQ = np.copy (self.Q)\n",
    "\n",
    "            for i in range (EXPLORE_COUNT):\n",
    "                state = random.choice (range (self.states - 1))\n",
    "\n",
    "                while (state != self.endState):\n",
    "                    # Exploration\n",
    "                    n_state = random.choice (self.nextStates [state])\n",
    "\n",
    "                    # Exploitation\n",
    "                    max_quality_next_state = -1\n",
    "                    for n_n_state in self.nextStates [n_state]:\n",
    "                        max_quality_next_state = max (max_quality_next_state, self.Q [n_state][n_n_state])\n",
    "\n",
    "                    # Update quality matrix\n",
    "                    self.Q [state][n_state] = self.R [state][n_state] + (gamma * max_quality_next_state)\n",
    "\n",
    "                    # Move to next state\n",
    "                    state = n_state\n",
    "            \n",
    "            # Normalize quality matrix\n",
    "            self.Q = self.Q / np.max (np.abs (self.Q))\n",
    "            improvement = np.sum (np.abs (self.Q - prevQ))\n",
    "\n",
    "            print \"  [TRAINING] Epoch: %2d | Improvement: %.3f\" % (epoch, improvement)\n",
    "\n",
    "            if improvement < improvement_threshold:\n",
    "                break\n",
    "\n",
    "        print \"[STATUS] Training Complete!\"\n",
    "        return\n",
    "\n",
    "    def play (self, start):\n",
    "        ''' The purpose of this function is to use a trained Q-agent for\n",
    "        solving the maze puzzle. The agent is given a starting point and it\n",
    "        must navigate the maze to find the best path to the goal i.e., treasure\n",
    "        while avoiding any obstacles. It is assumed that the maze is\n",
    "        solvable.'''\n",
    "        path = [start]\n",
    "        curPosition = start\n",
    "\n",
    "        while (path [-1] != self.treasure and len (path) < self.states):\n",
    "            nextCell = self.make_best_move (curPosition)\n",
    "            path.append (nextCell)\n",
    "            curPosition = nextCell\n",
    "\n",
    "        return path\n",
    "\n",
    "    def make_best_move (self, cell):\n",
    "        state = self.cell_to_state (cell)\n",
    "        qList = self.Q [state]\n",
    "        possibleNextStates = self.nextStates [state]\n",
    "\n",
    "        bestNextState = possibleNextStates [0]\n",
    "        bestQ = qList [bestNextState]\n",
    "\n",
    "        for pState in possibleNextStates:\n",
    "            stateQ = qList [pState]\n",
    "\n",
    "            if stateQ > bestQ:\n",
    "                bestNextState = pState\n",
    "                bestQ = stateQ\n",
    "\n",
    "        nextCell = self.state_to_cell (bestNextState)\n",
    "\n",
    "        return nextCell\n",
    "\n",
    "    def link_states (self, state, nextState):\n",
    "        if state not in self.nextStates.keys ():\n",
    "            self.nextStates [state] = []\n",
    "\n",
    "        if nextState not in self.nextStates.keys ():\n",
    "            self.nextStates [nextState] = []\n",
    "\n",
    "        self.nextStates [state].append (nextState)\n",
    "        self.nextStates [nextState].append (state)\n",
    "\n",
    "        return\n",
    "\n",
    "    def cell_to_state (self, cell):\n",
    "        state = cell [0] * self.rows + cell [1]\n",
    "        return state\n",
    "    \n",
    "    def state_to_cell (self, state):\n",
    "        x = state // self.rows\n",
    "        y = state % self.rows\n",
    "        cell = (x, y)\n",
    "        return cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sec-3: Demonstration\n",
    "---\n",
    "In this section, we demonstrate our learning algroithm by using it to solve a sample map.\n",
    "\n",
    "### Sec-3.1: An External Module for Visualizing the Game\n",
    "We are using an external module (please see **ACKNOWLEDGMENT** for source) for visualizing the board. In the gist below, a helper function is written to achieve this goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This file contains the external module that\n",
    "# we use for visualizing the board\n",
    "import painter\n",
    "import scipy.misc as smp\n",
    "\n",
    "# A very simple helper function for using the paint module\n",
    "# for visualizing the maze\n",
    "def visualize_board (board, start = None, path = None):\n",
    "    paint = painter.Paint (board.maze, board.goal, start, path)\n",
    "    pixels = paint.generate_image ()\n",
    "    image = smp.toimage (pixels)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sec-3.2.1: Game with 3x3 Board, 4 Obstacles and Hidden Treasure\n",
    "The initial setting of the board can be seen in the image following the code below. The **treasure** is annotated by the **green square**, the **obstacles** are visualized as **black walls**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJMAAACTCAIAAAAFh7xCAAABsElEQVR4nO3dMWoEMRAAwZG5/395\nHTg4sB354HbaVL1A0JpFgdDO0HRm5rquu5fxdM66JS10zvm4ew38kXJVylUpV6VclXJVylUpV6Vc\nlXJVylUpV6VclXJVylUpV6VclXJVylUpV6VclXJVylUpV6VclXJVylUpV6VclXJVylUpV6VclXJV\nylUpV6VclXJVylUpV6VclXJVylUpV6VclXJVj7sX8LuvtxJX2fZwo5mrWjdz27b2rPwAjJnrUq5K\nuSrlqpSrUq5KuSrlqpSrUq5KuSrlqpSrUq5KuSrlqpSrUq5KuSrlqpSrUq5KuSrlqpSrUq5KuSrl\nqpSrUq5KuSrlqpSrUq5KuSrlqpSrUq5KuSrlqpSrUq5KuSrlqpSrUq7qMStfAVz4VuI2Zq7qjA0e\ndM4xc1XKVSlXpVyVclXKVSlXpVyVclXKVSlXpVyVclXKVSlXpVyVclXKVSlXte4Puf/Sme+36655\n9e6PmatSrkq5KuWqlKtytnyH10+SP5m5KuWqlKtSrkq5KuWqlKtSrkq5KuWqlKtSrkq5KuWqlKtS\nrkq5KuWqlKtSDt7rEypnFynC7MYfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=147x147 at 0x10B16DD10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maze = [[(0, 0), (0, 2), (1, 2)],\n",
    "        [(0, 0), (1, 0), (2, 2)],\n",
    "        [(2, 1), (1, 1), (2, 1)]]\n",
    "\n",
    "treasure = (2, 2)\n",
    "\n",
    "board = Board (maze, treasure)\n",
    "visualize_board (board)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sec-3.2.2: Training the Q-Agent and Solving the Map\n",
    "In the following code, the reinforcement learning agent is trained on the map and then the trained agent is used to solve the map i.e., find the path to the treasure from a given starting point while avoiding the obstacles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STATUS] Training Q-agent...\n",
      "  [TRAINING] Epoch:  1 | Improvement: 7.499\n",
      "  [TRAINING] Epoch:  2 | Improvement: 0.410\n",
      "  [TRAINING] Epoch:  3 | Improvement: 0.000\n",
      "[STATUS] Training Complete!\n",
      "\n",
      "Optimal Path to the Treasure:  [(0, 0), (1, 0), (1, 1), (2, 1), (2, 2)]\n"
     ]
    }
   ],
   "source": [
    "# Create and train the agent and use it to play the game\n",
    "agent = QLearn (board)\n",
    "agent.train ()\n",
    "startPoint = (0, 0)\n",
    "path = agent.play (startPoint)\n",
    "\n",
    "print\n",
    "print \"Optimal Path to the Treasure: \", path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sec-3.2.3: Visualized the Solved Map\n",
    "The solved map with the path chosen by the agent is shown below. The **start point** is shows as a **red square** and **the path** consists of blocks marked as **grey squares** connecting the start point with the treasure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJMAAACTCAIAAAAFh7xCAAAB2UlEQVR4nO3cwUoDMRRA0UT63518\neV24ENudg51cOWdZKAQvj2YR3xg0zTHG/X6/+hjf1lpjjMfjcfVBtjbn/Lj6DPySclXKVSlXpVyV\nclXKVSlXpVyVclXKVSlXpVyVclXKVSlXpVyVclXKVd1Ofv9Y6/mTnV61/GNmrkq5KuWqlKtSrur0\n3dJN8iJmrkq5KuWqlKtSrkq5KuWqlKtSrkq5KuWqlKtSrkq5KuWqlKtSrkq5KuWqlKs6+4Loj8w5\nrz7Cs90WN5q5qu1mbqs1qV/Wy3+97MDMVSlXpVyVclXKVSlXpVyVclXKVSlXpVyVclXKVSlXpVyV\nclXKVSlXpVyVclXKVSlXpVyVclXKVSlXpVyVclXKVSlXpVyVclXKVSlXpVyVclXKVSlXpVyVclXK\nVSlXpVyVclXKVSlXdRtbbgHccFfibrbbb7mt4ziuPsIPc+y353a3v9GG1lp+56qUq1KuSrkq5aqU\nq1KuSrkq5aqUq1KuSrkq5aqUq1KuSrkq5aqUq1Kuyguid1jH8+u6+3H2cZuZq1KuSrkq5aqUq3K3\nfIfzN8lXZq5KuSrlqpSrUq5KuSrlqpSrUq5KuSrlqpSrUq5KuSrlqpSrUq5KuSrlqpSD9/oEMrcg\nrTSWArUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=147x147 at 0x10B180050>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualize_board (board, startPoint, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sec-4: Conclusion\n",
    "---\n",
    "It can be seen that **our trained agent is doing an excellent job in finding the treasure in the map while avoiding obstacles.** The path selected by our agent is the same as the one that would be chosen by a human player in solving this map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ACKNOWLEDGMENT**: The board visualization module was copied in entirety from the following source:\n",
    "https://github.com/mitchellspryn/QLearningMazeSolver/blob/master/maze.py\n",
    "\n",
    "We also consulted the same source for understanding the Q-learning algorithm.\n",
    "\n",
    "Despite help from the source mentioned above, the code written in this notebook was coded entirely by us. **We took the understanding we developed from the cited source to develop Q-learning pseudo-code and then wrote the code in this notebook from scratch.**\n",
    "\n",
    "This has been the **toughest and most demanding** assignment that we have done in this course. **We have spent > 15 hours for doing this assignment.**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
